{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sepsis Prediction Data Analysis & Preprocessing\n",
                "\n",
                "This notebook analyzes the unified parquet dataset, performs standardization, handles missing values, and creates data splits."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import os\n",
                "\n",
                "# Set plot style\n",
                "sns.set(style=\"whitegrid\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_path = \"../data/unified/unified_data.parquet\"\n",
                "df = pd.read_parquet(data_path)\n",
                "print(f\"Dataset Shape: {df.shape}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Analysis Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Column Types\n",
                "print(df.dtypes)\n",
                "\n",
                "# Unique Patients\n",
                "n_patients = df['PatientID'].nunique()\n",
                "print(f\"Number of unique patients: {n_patients}\")\n",
                "\n",
                "# Class Balance (SepsisLabel)\n",
                "sepsis_counts = df['SepsisLabel'].value_counts()\n",
                "print(\"\\nClass Balance (Rows):\")\n",
                "print(sepsis_counts)\n",
                "print(f\"Sepsis Prevalence (Rows): {sepsis_counts[1] / len(df):.2%}\")\n",
                "\n",
                "# Patient-level Class Balance\n",
                "patient_labels = df.groupby('PatientID')['SepsisLabel'].max()\n",
                "print(\"\\nClass Balance (Patients):\")\n",
                "print(patient_labels.value_counts())\n",
                "print(f\"Sepsis Prevalence (Patients): {patient_labels.sum() / n_patients:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Missing Values Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "missing_counts = df.isnull().sum()\n",
                "missing_pct = (missing_counts / len(df)) * 100\n",
                "\n",
                "missing_df = pd.DataFrame({'Missing Count': missing_counts, 'Missing %': missing_pct})\n",
                "missing_df = missing_df.sort_values(by='Missing %', ascending=False)\n",
                "\n",
                "plt.figure(figsize=(12, 8))\n",
                "sns.barplot(x=missing_df.index, y=missing_df['Missing %'])\n",
                "plt.xticks(rotation=90)\n",
                "plt.title(\"Percentage of Missing Values per Column\")\n",
                "plt.ylabel(\"%\")\n",
                "plt.show()\n",
                "\n",
                "missing_df.head(20)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Splitting\n",
                "We split the data by PatientID to ensure no leakage. We will create Train (70%), Validation (15%), and Test (15%) splits."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "patient_ids = df['PatientID'].unique()\n",
                "train_ids, temp_ids = train_test_split(patient_ids, test_size=0.3, random_state=42)\n",
                "val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=42)\n",
                "\n",
                "print(f\"Train Patients: {len(train_ids)}\")\n",
                "print(f\"Val Patients: {len(val_ids)}\")\n",
                "print(f\"Test Patients: {len(test_ids)}\")\n",
                "\n",
                "# Create masks for splitting\n",
                "train_mask = df['PatientID'].isin(train_ids)\n",
                "val_mask = df['PatientID'].isin(val_ids)\n",
                "test_mask = df['PatientID'].isin(test_ids)\n",
                "\n",
                "train_df = df[train_mask].copy()\n",
                "val_df = df[val_mask].copy()\n",
                "test_df = df[test_mask].copy()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Standardization & Missingness Handling\n",
                "We calculate mean and std ONLY on the training set. We also create mask features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify feature columns (exclude IDs and Labels)\n",
                "feature_cols = [c for c in df.columns if c not in ['PatientID', 'SepsisLabel', 'Unit1', 'Unit2', 'HospAdmTime', 'ICULOS']]\n",
                "print(f\"Feature Columns: {feature_cols}\")\n",
                "\n",
                "# Compute statistics on Train\n",
                "train_mean = train_df[feature_cols].mean()\n",
                "train_std = train_df[feature_cols].std()\n",
                "\n",
                "# Avoid division by zero\n",
                "train_std = train_std.replace(0, 1.0)\n",
                "\n",
                "def preprocess_split(split_df, mean, std, feature_cols):\n",
                "    # 1. Create Mask (1 = Observed, 0 = Missing)\n",
                "    mask = (~split_df[feature_cols].isnull()).astype(int)\n",
                "    mask.columns = [f\"{c}_mask\" for c in feature_cols]\n",
                "    \n",
                "    # 2. Forward Fill (and fill remaining NaNs with 0 after standardization, or mean before)\n",
                "    # Standard practice: Forward fill, then fill remaining with mean (0 after standardization)\n",
                "    # Actually, let's just fill with mean (0) for now for the values, but keep the mask.\n",
                "    # Or better: Forward Fill per patient. \n",
                "    # Groupby fillna is slow. Let's do global fillna(0) for simple baseline, \n",
                "    # but for time-series, forward fill is better.\n",
                "    \n",
                "    # For efficiency in this notebook, we'll use simple imputation + mask.\n",
                "    # (Refining to per-patient forward fill would be better in the full pipeline)\n",
                "    \n",
                "    # Standardization\n",
                "    scaled_features = (split_df[feature_cols] - mean) / std\n",
                "    \n",
                "    # Fill NaNs with 0 (which is the mean)\n",
                "    scaled_features = scaled_features.fillna(0)\n",
                "    \n",
                "    # Concatenate: PatientID, Scaled Features, Masks, Other Cols, Label\n",
                "    result = pd.concat([\n",
                "        split_df[['PatientID', 'ICULOS', 'Unit1', 'Unit2', 'HospAdmTime']],\n",
                "        scaled_features,\n",
                "        mask,\n",
                "        split_df[['SepsisLabel']]\n",
                "    ], axis=1)\n",
                "    \n",
                "    return result\n",
                "\n",
                "print(\"Processing Train...\")\n",
                "train_processed = preprocess_split(train_df, train_mean, train_std, feature_cols)\n",
                "\n",
                "print(\"Processing Val...\")\n",
                "val_processed = preprocess_split(val_df, train_mean, train_std, feature_cols)\n",
                "\n",
                "print(\"Processing Test...\")\n",
                "test_processed = preprocess_split(test_df, train_mean, train_std, feature_cols)\n",
                "\n",
                "train_processed.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Save Processed Splits"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "output_dir = \"../data/processed_splits\"\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "\n",
                "train_processed.to_parquet(os.path.join(output_dir, \"train.parquet\"))\n",
                "val_processed.to_parquet(os.path.join(output_dir, \"val.parquet\"))\n",
                "test_processed.to_parquet(os.path.join(output_dir, \"test.parquet\"))\n",
                "\n",
                "print(f\"Saved splits to {output_dir}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}